{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library used and deeclare data needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from numpy.typing import NDArray\n",
    "from scipy.stats import zscore\n",
    "from pytictoc import TicToc\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -4.336154e-16\n",
      "1    2.178551e-16\n",
      "dtype: float64\n",
      "hahah\n",
      "0    1.427663\n",
      "1    1.888831\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "kernel_fcn = 'rbf' # is this gaussian?\n",
    "opts_csv_fold = 5\n",
    "with open('./data/algolabels.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            algo_labels = row\n",
    "\n",
    "y = np.loadtxt('./data/ybin.csv', delimiter=',', skiprows=1)\n",
    "z = pd.read_csv('./data/z_M.csv', header=None, dtype=np.float64)\n",
    "\n",
    "ninst, nalgos = y.shape\n",
    "w = np.ones((ninst, nalgos))\n",
    "\n",
    "\"\"\" Problem 1 - Normalization generate different result with MATLAB \"\"\"\n",
    "# TODO: Lam: review\n",
    "# Matlab Mean: -2.97455980182589e-16,-1.42443708819831e-16\n",
    "# Matlab SD: sigma: 1.42766287722398,1.88883050190813\n",
    "z_norm = (z-np.mean(z, axis=0))/np.std(z, ddof=1, axis=0)\n",
    "print(np.std(z, ddof=1, axis=0))\n",
    "pd.DataFrame(z_norm).to_csv('z_f.csv', header=None, index=None)\n",
    "# scaler = StandardScaler().fit(z)\n",
    "\n",
    "cvcmat = np.zeros((nalgos, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_norm_M = pd.read_csv('./data/z_norm_M.csv', header=None, dtype=np.float64)\n",
    "z_norm_P = pd.read_csv('z_f.csv', header=None, dtype=np.float64)\n",
    "\n",
    "tolerance = 1e-10\n",
    "are_close = np.isclose(z_norm_M.values, z_norm_P.values, atol=tolerance)\n",
    "results = are_close.all()\n",
    "\n",
    "print(\"Are all elements close within the tolerance level: \", results)\n",
    "if not results:\n",
    "    mismatches = np.where(~are_close)\n",
    "    print(\"Mismatch found at positions:\", mismatches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_libsvm(z, y, kkv, kernel_given):\n",
    "    accuracy= dict()\n",
    "    for k, v in kkv.items():\n",
    "        train_index, test_index = v[0], v[1]\n",
    "        # prepare training data\n",
    "        x_train = [z[i] for i in train_index]\n",
    "        y_train = [y[i] for i in train_index]\n",
    "        # prepare test data\n",
    "        x_test = [z[i] for i in test_index]\n",
    "        y_test = [y[i] for i in test_index]\n",
    "        svm = SVC(kernel=kernel_given, C=1.0, random_state = 0)\n",
    "        svm.fit(x_train, y_train)\n",
    "        y_pred = svm.predict(x_test)\n",
    "        # calculate accuracy\n",
    "        accuracy[k] = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_matsvm(z, y, w, skf, kernel_given, params):\n",
    "    # TODO Set up parallel workers in pool\n",
    "    \n",
    "\n",
    "    # Scikit-learn lib need to ensuring data contiguity\n",
    "    z = np.ascontiguousarray(z)\n",
    "    y = np.ascontiguousarray(y)\n",
    "    w = np.ascontiguousarray(w)\n",
    "    \n",
    "    # Check if hyperparameter is given by user\n",
    "    if(np.isnan(params)):\n",
    "\n",
    "        # Initialize a random number generator\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Retrieve default hyperparameters for fitcsvm and sets the range for the box constraint (C) and kernel scale\n",
    "        # Define the range for C and gamma in a logarithmic scale\n",
    "        param_grid = {\n",
    "        'C': np.logspace(-10, 4, base=2, num=15),\n",
    "        'gamma': np.logspace(-10, 4, base=2, num=15)\n",
    "        }\n",
    "\n",
    "        # By default, the class_weight=None represent equal weight\n",
    "        svm_model = SVC(kernel=kernel_given, class_weight=None, random_state=0)\n",
    "\n",
    "        # steps = list()\n",
    "        # steps.append(('scaler', StandardScaler()))\n",
    "        # steps.append(('model', svm_model))\n",
    "        # pipeline = Pipeline(steps=steps)\n",
    "\n",
    "        # Used for exhaustive search over specified parameter values for the SVM. The param_grid defines \n",
    "        # the range over which C and gamma will be tuned.\n",
    "        # GridSearchCV for optimizing the hyperparameters\n",
    "        # TODO Lam: Figure out a better method\n",
    "        grid_search = GridSearchCV(\n",
    "            svm_model, param_grid, \n",
    "            scoring='accuracy', # 'roc_auc'\n",
    "            cv=skf, \n",
    "            verbose=0\n",
    "            #, n_jobs=nworkers if nworkers != 0 else None,\n",
    "            )\n",
    "        \n",
    "        # OPT1: \n",
    "        # Fit a probability calibration model with trained SVM\n",
    "        # print(z.shape, y.shape, w.shape)\n",
    "        # TODO Lam: no matter what c, gamma or kfold, but the best_svm should be same \n",
    "        grid_search.fit(z, y, sample_weight=w)   # Fit GridSearchCV\n",
    "        best_svm = grid_search.best_estimator_\n",
    "       \n",
    "        calibrator = CalibratedClassifierCV(best_svm, cv='prefit', method='sigmoid')\n",
    "        calibrator.fit(z, y, sample_weight=w)\n",
    "\n",
    "        # OPT2: retrain\n",
    "        # calibrator = CalibratedClassifierCV(best_svm, cv=skf, method='sigmoid')\n",
    "        # calibrator.fit(z_norm, y, sample_weight=w)\n",
    "\n",
    "        best_C = grid_search.best_params_['C']\n",
    "        best_g = grid_search.best_params_['gamma']\n",
    "\n",
    "        y_sub = calibrator.predict(z)\n",
    "        p_sub = calibrator.predict_proba(z)\n",
    "\n",
    "        # Making predictions on the same data to simulate resubstitution prediction\n",
    "        y_hat = y_sub\n",
    "        p_hat = p_sub\n",
    "        \n",
    "        print(\"Best C:\", best_C)\n",
    "        print(\"Best gamma:\", best_g)\n",
    "\n",
    "\n",
    "    return best_svm, y_sub, p_sub, y_hat, p_hat, best_C, best_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATSVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TicToc()\n",
    "t.tic()\n",
    "\n",
    "for i in range(nalgos):\n",
    "    t_inner = TicToc()\n",
    "    t_inner.tic()\n",
    "\n",
    "    state = np.random.get_state()\n",
    "    np.random.seed(0)  # equivalent to MATLAB's rng('default') ?\n",
    "\n",
    "    # y_b = [row[i] for row in y]\n",
    "    y_b = y[:, i]\n",
    "\n",
    "    \n",
    "    # REQUIRE: Test case for validation the result\n",
    "    # shuffle = True means that it partitions the data as it is presented. If the data is already in a random \n",
    "    # order, this isn't typically an issue, but if the data has some sort of order (e.g., all instances of one \n",
    "    # class followed by all instances of another), stratification alone won't randomize the order of the instances within each fold.\n",
    "\n",
    "    # skf = StratifiedKFold(n_splits = opts_csv_fold, shuffle = True, random_state = 0)\n",
    "    # TODO Xin : test it (exact match is not required, but difference has to be minimise)\n",
    "    skf = KFold(n_splits = opts_csv_fold, shuffle = True, random_state = 0)\n",
    "    \n",
    "    # Test k-fold cross validation\n",
    "    # data_splits = skf.split(z_norm, y_b)\n",
    "    # test_split = next(data_splits)\n",
    "    # print(\"Train indices: \", test_split[0][0:100])\n",
    "    # print(\"Test indices: \", test_split[1][0:100])\n",
    "\n",
    "    # fit_matsvm(z, y_b, w[:, i], skf, kernel_fcn, np.nan)\n",
    "\n",
    "    best_svm, y_sub, p_sub, y_hat, p_hat, best_C, best_g = fit_matsvm(z, y_b, w[:, i], skf, kernel_fcn, np.nan)\n",
    "    aux = confusion_matrix(y_b, y_sub)\n",
    "\n",
    "    # np.prod(aux.shape) != 4 is False\n",
    "    cvcmat[i, :] = aux.flatten()\n",
    "    models_left = nalgos - (i + 1)\n",
    "    print(f\"    -> PYTHIA has trained a model for {algo_labels[i]}, there are {models_left} models left to train.\")\n",
    "\n",
    "    print(f\"      -> Elapsed time: {t_inner.tocvalue():.2f}s\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "tn, fp, fn, tp = cvcmat[:, 0], cvcmat[:, 1], cvcmat[:, 2], cvcmat[:, 3]\n",
    "print(tn.dtype)  \n",
    "print(tn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "accuracy = (tp + tn) / ninst\n",
    "\n",
    "print(precision)\n",
    "# print(recall)\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('./data/ybin.csv')\n",
    "y = y.values.tolist()\n",
    "\n",
    "z = pd.read_csv('./data/z.csv').values.tolist()\n",
    "z_norm = zscore(z, axis = 0, ddof = 1)\n",
    "\n",
    "for i in range(nalgos):\n",
    "    t_inner = TicToc()\n",
    "    t_inner.tic()\n",
    "\n",
    "    state = np.random.get_state()\n",
    "    np.random.seed(0)  # equivalent to MATLAB's rng('default') ?\n",
    "\n",
    "    # REQUIRE: Test case for validation the result\n",
    "    y_b = [row[i] for row in y]\n",
    "    # y_b = y[:, i]\n",
    "    skf = StratifiedKFold(n_splits = opts_csv_fold, shuffle = True, random_state = 0)\n",
    "    \n",
    "    kkv= dict()\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(np.zeros(len(y_b)), y_b)):\n",
    "        kkv[i] = [train_index.tolist(), test_index.tolist()]\n",
    "    \n",
    "    # start training using svm\n",
    "    svm_res = fit_libsvm(z_norm, y_b, kkv, kernel_fcn)\n",
    "\n",
    "    # visualise accuracy score\n",
    "for k, v in svm_res.items():\n",
    "    print(f'{k} fold: accuracy score = {v}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8d4e9334dcc093c5455b6f55ea09f9028125097d0b5d072b5d027a0ea5ecbe2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.19 ('new_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
