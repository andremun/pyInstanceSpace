{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library used and deeclare data needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from numpy.typing import NDArray\n",
    "from scipy.stats import zscore\n",
    "from pytictoc import TicToc\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from matilda.data.model import AlgorithmSummary\n",
    "# from matilda.data.option import Opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_fcn = 'rbf'\n",
    "opts_csv_fold = 5\n",
    "with open('./data/algolabels.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            algo_labels = row\n",
    "\n",
    "# prepare for y, transpose it\n",
    "# y = pd.read_csv('./data/ybin.csv')\n",
    "# y = y.values.tolist()\n",
    "\n",
    "y = np.loadtxt('./data/ybin.csv', delimiter=',', skiprows=1)\n",
    "z = pd.read_csv('./data/z.csv')\n",
    "\n",
    "ninst, nalgos = y.shape\n",
    "w = np.ones((ninst, nalgos))\n",
    "# prepare for z, normalise it\n",
    "\n",
    "z_norm = zscore(z, axis = 0, ddof = 1)\n",
    "\n",
    "cvcmat = np.zeros((nalgos, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_libsvm(z, y, kkv, kernel_given):\n",
    "    accuracy= dict()\n",
    "    for k, v in kkv.items():\n",
    "        train_index, test_index = v[0], v[1]\n",
    "        # prepare training data\n",
    "        x_train = [z[i] for i in train_index]\n",
    "        y_train = [y[i] for i in train_index]\n",
    "        # prepare test data\n",
    "        x_test = [z[i] for i in test_index]\n",
    "        y_test = [y[i] for i in test_index]\n",
    "        svm = SVC(kernel=kernel_given, C=1.0, random_state = 0)\n",
    "        svm.fit(x_train, y_train)\n",
    "        y_pred = svm.predict(x_test)\n",
    "        # calculate accuracy\n",
    "        accuracy[k] = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_matsvm(z, y, w, skf, kernel_given, params):\n",
    "    # TODO Set up parallel workers in pool\n",
    "    \n",
    "\n",
    "    # Scikit-learn lib need to ensuring data contiguity\n",
    "    z = np.ascontiguousarray(z)\n",
    "    y = np.ascontiguousarray(y)\n",
    "    w = np.ascontiguousarray(w)\n",
    "    \n",
    "    # Check if hyperparameter is given by user\n",
    "    if(np.isnan(params)):\n",
    "        # Initialize a random number generator\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Retrieve default hyperparameters for fitcsvm and sets the range for the box constraint (C) and kernel scale\n",
    "        # Define the range for C and gamma in a logarithmic scale\n",
    "        param_grid = {\n",
    "        # Generates 15 numbers between 2^-10 and 2^4\n",
    "        'C': np.logspace(-10, 4, base=2, num=15),\n",
    "        'gamma': np.logspace(-10, 4, base=2, num=15)\n",
    "        }\n",
    "\n",
    "        # z is normalised without modifying the scale, since in the original settings, \n",
    "        # the 'Standardize'is false\n",
    "        # MinMaxScaler  --- slight improve!\n",
    "        # scaler = MinMaxScaler()\n",
    "        # scaler.fit(z)\n",
    "        # z = scaler.transform(z)\n",
    "\n",
    "        # By default, the class_weight=None represent equal weight OR\n",
    "        # Let SVC balance the weight with class_weight='balance'    ---------?\n",
    "        svm_model = SVC(kernel=kernel_given, class_weight=None, random_state=0, probability=True)\n",
    "\n",
    "        # scores = cross_val_score(model, z, y, scoring='accuracy', cv=skf)\n",
    "        # # for score in scores:\n",
    "        # #     print(\"Accuracy for this al is: \", accuracy)\n",
    "        # print(\"Mean Accuracy for this al is: \", np.mean(scores))\n",
    "\n",
    "\n",
    "        # Used for exhaustive search over specified parameter values for the SVM. The param_grid defines \n",
    "        # the range over which C and gamma will be tuned.\n",
    "        # GridSearchCV for optimizing the hyperparameters\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=svm_model, \n",
    "            param_grid=param_grid, \n",
    "            # 'roc_auc' measures the area under the receiver operating characteristic curve, which is a \n",
    "            # good choice for binary classification problems, especially with imbalanced classes.\n",
    "            scoring='accuracy', # 'roc_auc'\n",
    "            cv=skf, \n",
    "            verbose=0\n",
    "            #, n_jobs=nworkers if nworkers != 0 else None,\n",
    "            )\n",
    "\n",
    "        # OPT1: \n",
    "        # Fit a probability calibration model with trained SVM\n",
    "   \n",
    "        grid_search.fit(z, y, sample_weight=w)   # Fit GridSearchCV\n",
    "        best_svm = grid_search.best_estimator_\n",
    "        # With cv='prefit' and default method is method='sigmoid'\n",
    "        # calibrator = CalibratedClassifierCV(best_svm, cv='prefit', method='sigmoid')\n",
    "        # calibrator.fit(z, y, sample_weight=w)\n",
    "\n",
    "        # OPT2: Use it to train\n",
    "        # calibrator = CalibratedClassifierCV(best_svm, cv=skf, method='sigmoid')\n",
    "        # calibrator.fit(z, y, sample_weight=w)\n",
    "\n",
    "        # Retrieve the best model and hyperparameters\n",
    "        \n",
    "        best_C = grid_search.best_params_['C']\n",
    "        best_g = grid_search.best_params_['gamma']\n",
    "\n",
    "        # y_sub = best_svm.predict(z)\n",
    "        y_sub = best_svm.predict(z)\n",
    "        p_sub = best_svm.predict_proba(z)\n",
    "\n",
    "        # Making predictions on the same data to simulate resubstitution prediction\n",
    "        y_hat = y_sub\n",
    "        p_hat = p_sub\n",
    "        \n",
    "        print(\"Best C:\", best_C)\n",
    "        print(\"Best gamma:\", best_g)\n",
    "\n",
    "\n",
    "    return best_svm, y_sub, p_sub, y_hat, p_hat, best_C, best_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATSVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 0.5\n",
      "Best gamma: 0.125\n",
      "Best C: 0.5\n",
      "Best gamma: 0.125\n",
      "------------aux-----------\n",
      "[[ 65  17]\n",
      " [ 16 113]]\n",
      "    -> PYTHIA has trained a model for NB, there are 9 models left to train.\n",
      "      -> Elapsed time: 23.34s\n",
      "Best C: 4.0\n",
      "Best gamma: 2.0\n",
      "Best C: 4.0\n",
      "Best gamma: 2.0\n",
      "------------aux-----------\n",
      "[[ 63  20]\n",
      " [  7 121]]\n",
      "    -> PYTHIA has trained a model for LDA, there are 8 models left to train.\n",
      "      -> Elapsed time: 23.88s\n",
      "Best C: 8.0\n",
      "Best gamma: 0.125\n",
      "Best C: 8.0\n",
      "Best gamma: 0.125\n",
      "------------aux-----------\n",
      "[[114  22]\n",
      " [ 20  55]]\n",
      "    -> PYTHIA has trained a model for QDA, there are 7 models left to train.\n",
      "      -> Elapsed time: 23.70s\n",
      "Best C: 1.0\n",
      "Best gamma: 0.5\n",
      "Best C: 1.0\n",
      "Best gamma: 0.5\n",
      "------------aux-----------\n",
      "[[ 54   6]\n",
      " [ 10 141]]\n",
      "    -> PYTHIA has trained a model for CART, there are 6 models left to train.\n",
      "      -> Elapsed time: 18.86s\n",
      "Best C: 16.0\n",
      "Best gamma: 0.0625\n",
      "Best C: 16.0\n",
      "Best gamma: 0.0625\n",
      "------------aux-----------\n",
      "[[ 49   8]\n",
      " [ 10 144]]\n",
      "    -> PYTHIA has trained a model for J48, there are 5 models left to train.\n",
      "      -> Elapsed time: 18.97s\n",
      "Best C: 0.5\n",
      "Best gamma: 0.5\n",
      "Best C: 0.5\n",
      "Best gamma: 0.5\n",
      "------------aux-----------\n",
      "[[ 49   8]\n",
      " [ 10 144]]\n",
      "    -> PYTHIA has trained a model for KNN, there are 4 models left to train.\n",
      "      -> Elapsed time: 19.41s\n",
      "Best C: 0.25\n",
      "Best gamma: 0.5\n",
      "Best C: 0.25\n",
      "Best gamma: 0.5\n",
      "------------aux-----------\n",
      "[[ 54   6]\n",
      " [  8 143]]\n",
      "    -> PYTHIA has trained a model for L_SVM, there are 3 models left to train.\n",
      "      -> Elapsed time: 19.54s\n",
      "Best C: 0.25\n",
      "Best gamma: 2.0\n",
      "Best C: 0.25\n",
      "Best gamma: 2.0\n",
      "------------aux-----------\n",
      "[[ 85  14]\n",
      " [  8 104]]\n",
      "    -> PYTHIA has trained a model for poly_SVM, there are 2 models left to train.\n",
      "      -> Elapsed time: 26.57s\n",
      "Best C: 16.0\n",
      "Best gamma: 8.0\n",
      "Best C: 16.0\n",
      "Best gamma: 8.0\n",
      "------------aux-----------\n",
      "[[ 59   2]\n",
      " [  4 146]]\n",
      "    -> PYTHIA has trained a model for RBF_SVM, there are 1 models left to train.\n",
      "      -> Elapsed time: 19.47s\n",
      "Best C: 2.0\n",
      "Best gamma: 8.0\n",
      "Best C: 2.0\n",
      "Best gamma: 8.0\n",
      "------------aux-----------\n",
      "[[ 57   9]\n",
      " [  7 138]]\n",
      "    -> PYTHIA has trained a model for RandF, there are 0 models left to train.\n",
      "      -> Elapsed time: 21.37s\n"
     ]
    }
   ],
   "source": [
    "t = TicToc()\n",
    "t.tic()\n",
    "\n",
    "for i in range(nalgos):\n",
    "    t_inner = TicToc()\n",
    "    t_inner.tic()\n",
    "\n",
    "    state = np.random.get_state()\n",
    "    np.random.seed(0)  # equivalent to MATLAB's rng('default') ?\n",
    "\n",
    "    # y_b = [row[i] for row in y]\n",
    "    y_b = y[:, i]\n",
    "\n",
    "    # Split data into train and test to verify accuracy after fitting the model\n",
    "    # Note: z is used here, not z_norm. If normalise and scale ahead, some test data might leak into\n",
    "    # the training process;\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(z, y_b, \n",
    "    #                                                     test_size=0.25,\n",
    "    #                                                     random_state=42) # =0?\n",
    "    \n",
    "    # REQUIRE: Test case for validation the result --better!\n",
    "    skf = StratifiedKFold(n_splits = opts_csv_fold, shuffle = True, random_state = 0)\n",
    "    # skf = KFold(n_splits = opts_csv_fold, shuffle = True, random_state = 0)\n",
    "    \n",
    "    # Test k-fold cross validation\n",
    "    # data_splits = skf.split(z_norm, y_b)\n",
    "    # test_split = next(data_splits)\n",
    "    # print(\"Train indices: \", test_split[0][0:100])\n",
    "    # print(\"Test indices: \", test_split[1][0:100])\n",
    "\n",
    "    fit_matsvm(z_norm, y_b, w[:, i], skf, kernel_fcn, np.nan)\n",
    "\n",
    "    best_svm, y_sub, p_sub, y_hat, p_hat, best_C, best_g = fit_matsvm(z_norm, y_b, w[:, i], skf, kernel_fcn, np.nan)\n",
    "    aux = confusion_matrix(y_b, y_sub)\n",
    "    print(\"------------aux-----------\")\n",
    "    print(aux)\n",
    "    # 66    17   16   113\n",
    "\n",
    "    # np.prod(aux.shape) != 4 is False\n",
    "    cvcmat[i, :] = aux.flatten()\n",
    "    models_left = nalgos - (i + 1)\n",
    "    print(f\"    -> PYTHIA has trained a model for {algo_labels[i]}, there are {models_left} models left to train.\")\n",
    "\n",
    "    print(f\"      -> Elapsed time: {t_inner.tocvalue():.2f}s\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "[ 65.  63. 114.  54.  49.  49.  54.  85.  59.  57.]\n",
      "[0.86923077 0.85815603 0.71428571 0.95918367 0.94736842 0.94736842\n",
      " 0.95973154 0.88135593 0.98648649 0.93877551]\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "tn, fp, fn, tp = cvcmat[:, 0], cvcmat[:, 1], cvcmat[:, 2], cvcmat[:, 3]\n",
    "print(tn.dtype)  \n",
    "print(tn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "accuracy = (tp + tn) / ninst\n",
    "\n",
    "print(precision)\n",
    "# print(recall)\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: accuracy score = 0.8837209302325582\n",
      "1 fold: accuracy score = 0.9047619047619048\n",
      "2 fold: accuracy score = 0.8333333333333334\n",
      "3 fold: accuracy score = 0.8333333333333334\n",
      "4 fold: accuracy score = 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "y = pd.read_csv('./data/ybin.csv')\n",
    "y = y.values.tolist()\n",
    "\n",
    "z = pd.read_csv('./data/z.csv').values.tolist()\n",
    "z_norm = zscore(z, axis = 0, ddof = 1)\n",
    "\n",
    "for i in range(nalgos):\n",
    "    t_inner = TicToc()\n",
    "    t_inner.tic()\n",
    "\n",
    "    state = np.random.get_state()\n",
    "    np.random.seed(0)  # equivalent to MATLAB's rng('default') ?\n",
    "\n",
    "    # REQUIRE: Test case for validation the result\n",
    "    y_b = [row[i] for row in y]\n",
    "    # y_b = y[:, i]\n",
    "    skf = StratifiedKFold(n_splits = opts_csv_fold, shuffle = True, random_state = 0)\n",
    "    \n",
    "    kkv= dict()\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(np.zeros(len(y_b)), y_b)):\n",
    "        kkv[i] = [train_index.tolist(), test_index.tolist()]\n",
    "    \n",
    "    # start training using svm\n",
    "    svm_res = fit_libsvm(z_norm, y_b, kkv, kernel_fcn)\n",
    "\n",
    "    # visualise accuracy score\n",
    "for k, v in svm_res.items():\n",
    "    print(f'{k} fold: accuracy score = {v}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8d4e9334dcc093c5455b6f55ea09f9028125097d0b5d072b5d027a0ea5ecbe2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.19 ('new_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
