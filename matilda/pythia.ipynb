{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library used and deeclare data needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from numpy.typing import NDArray\n",
    "from scipy.stats import zscore\n",
    "from pytictoc import TicToc\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold, cross_val_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from matilda.data.model import AlgorithmSummary\n",
    "# from matilda.data.option import Opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_fcn = 'rbf'\n",
    "opts_csv_fold = 5\n",
    "with open('./data/algolabels.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            algo_labels = row\n",
    "\n",
    "# prepare for y, transpose it\n",
    "# y = pd.read_csv('./data/ybin.csv')\n",
    "# y = y.values.tolist()\n",
    "\n",
    "y = np.loadtxt('./data/ybin.csv', delimiter=',', skiprows=1)\n",
    "z = pd.read_csv('./data/z.csv')\n",
    "\n",
    "ninst, nalgos = y.shape\n",
    "w = np.ones((ninst, nalgos))\n",
    "# prepare for z, normalise it\n",
    "\n",
    "# z_norm = zscore(z, axis = 0, ddof = 1)\n",
    "\n",
    "cvcmat = np.zeros((nalgos, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_libsvm(z, y, kkv, kernel_given):\n",
    "    accuracy= dict()\n",
    "    for k, v in kkv.items():\n",
    "        train_index, test_index = v[0], v[1]\n",
    "        # prepare training data\n",
    "        x_train = [z[i] for i in train_index]\n",
    "        y_train = [y[i] for i in train_index]\n",
    "        # prepare test data\n",
    "        x_test = [z[i] for i in test_index]\n",
    "        y_test = [y[i] for i in test_index]\n",
    "        svm = SVC(kernel=kernel_given, C=1.0, random_state = 0)\n",
    "        svm.fit(x_train, y_train)\n",
    "        y_pred = svm.predict(x_test)\n",
    "        # calculate accuracy\n",
    "        accuracy[k] = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_matsvm(z, y, w, skf, kernel_given, params):\n",
    "    # TODO Set up parallel workers in pool\n",
    "    \n",
    "\n",
    "    # Scikit-learn lib need to ensuring data contiguity\n",
    "    z = np.ascontiguousarray(z)\n",
    "    y = np.ascontiguousarray(y)\n",
    "    w = np.ascontiguousarray(w)\n",
    "    \n",
    "    # Check if hyperparameter is given by user\n",
    "    if(np.isnan(params)):\n",
    "        # Initialize a random number generator\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # Retrieve default hyperparameters for fitcsvm and sets the range for the box constraint (C) and kernel scale\n",
    "        # Define the range for C and gamma in a logarithmic scale\n",
    "        param_grid = {\n",
    "        # Generates 15 numbers between 2^-10 and 2^4\n",
    "        'C': np.logspace(-10, 4, base=2, num=15),\n",
    "        'gamma': np.logspace(-10, 4, base=2, num=15)\n",
    "        }\n",
    "\n",
    "        # z is normalised without modifying the scale, since in the original settings, \n",
    "        # the 'Standardize'is false\n",
    "        # MinMaxScaler  --- slight improve!\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(z)\n",
    "        z_norm = scaler.transform(z)\n",
    "\n",
    "        # By default, the class_weight=None represent equal weight OR\n",
    "        # Let SVC balance the weight with class_weight='balance'    ---------?\n",
    "        svm_model = SVC(kernel=kernel_given, class_weight=None, random_state=0)\n",
    "\n",
    "        # scores = cross_val_score(model, z, y, scoring='accuracy', cv=skf)\n",
    "        # # for score in scores:\n",
    "        # #     print(\"Accuracy for this al is: \", accuracy)\n",
    "        # print(\"Mean Accuracy for this al is: \", np.mean(scores))\n",
    "\n",
    "\n",
    "        # Used for exhaustive search over specified parameter values for the SVM. The param_grid defines \n",
    "        # the range over which C and gamma will be tuned.\n",
    "        # GridSearchCV for optimizing the hyperparameters\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=svm_model, \n",
    "            param_grid=param_grid, \n",
    "            # 'roc_auc' measures the area under the receiver operating characteristic curve, which is a \n",
    "            # good choice for binary classification problems, especially with imbalanced classes.\n",
    "            scoring='accuracy', # 'roc_auc'\n",
    "            cv=skf, \n",
    "            verbose=0\n",
    "            #, n_jobs=nworkers if nworkers != 0 else None,\n",
    "            )\n",
    "\n",
    "        # OPT1: \n",
    "        # Fit a probability calibration model with trained SVM\n",
    "        print(z.shape, y.shape, w.shape)\n",
    "        grid_search.fit(z_norm, y, sample_weight=w)   # Fit GridSearchCV\n",
    "        best_svm = grid_search.best_estimator_\n",
    "        # With cv='prefit' and default method is method='sigmoid'\n",
    "        calibrator = CalibratedClassifierCV(best_svm, cv='prefit', method='sigmoid')\n",
    "        calibrator.fit(z_norm, y, sample_weight=w)\n",
    "\n",
    "        # OPT2: Use it to train\n",
    "        # calibrator = CalibratedClassifierCV(best_svm, cv=skf, method='sigmoid')\n",
    "        # calibrator.fit(z_norm, y, sample_weight=w)\n",
    "\n",
    "        # Retrieve the best model and hyperparameters\n",
    "        \n",
    "        best_C = grid_search.best_params_['C']\n",
    "        best_g = grid_search.best_params_['gamma']\n",
    "\n",
    "        # y_sub = best_svm.predict(z)\n",
    "        y_sub = calibrator.predict(z)\n",
    "        p_sub = calibrator.predict_proba(z)\n",
    "\n",
    "        # Making predictions on the same data to simulate resubstitution prediction\n",
    "        y_hat = y_sub\n",
    "        p_hat = p_sub\n",
    "        \n",
    "        print(\"Best C:\", best_C)\n",
    "        print(\"Best gamma:\", best_g)\n",
    "\n",
    "\n",
    "    return best_svm, y_sub, p_sub, y_hat, p_hat, best_C, best_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MATSVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_bin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 31\u001b[0m\n\u001b[1;32m     22\u001b[0m skf \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits \u001b[38;5;241m=\u001b[39m opts_csv_fold, shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# skf = KFold(n_splits = opts_csv_fold, shuffle = True, random_state = 0)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Test k-fold cross validation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# print(\"Train indices: \", test_split[0][0:100])\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# print(\"Test indices: \", test_split[1][0:100])\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mfit_matsvm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_fcn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m best_svm, y_sub, p_sub, y_hat, p_hat, best_C, best_g \u001b[38;5;241m=\u001b[39m fit_matsvm(z, y_b, w[:, i], skf, kernel_fcn, np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m     34\u001b[0m aux \u001b[38;5;241m=\u001b[39m confusion_matrix(y_b, y_sub)\n",
      "Cell \u001b[0;32mIn[109], line 56\u001b[0m, in \u001b[0;36mfit_matsvm\u001b[0;34m(z, y, w, skf, kernel_given, params)\u001b[0m\n\u001b[1;32m     43\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[1;32m     44\u001b[0m     estimator\u001b[38;5;241m=\u001b[39msvm_model, \n\u001b[1;32m     45\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m#, n_jobs=nworkers if nworkers != 0 else None,\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     )\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# OPT1: \u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Fit a probability calibration model with trained SVM\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mprint\u001b[39m(z\u001b[38;5;241m.\u001b[39mshape, \u001b[43my_bin\u001b[49m\u001b[38;5;241m.\u001b[39mshape, w\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     57\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(z_norm, y, sample_weight\u001b[38;5;241m=\u001b[39mw)   \u001b[38;5;66;03m# Fit GridSearchCV\u001b[39;00m\n\u001b[1;32m     58\u001b[0m best_svm \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_bin' is not defined"
     ]
    }
   ],
   "source": [
    "t = TicToc()\n",
    "t.tic()\n",
    "\n",
    "for i in range(nalgos):\n",
    "    t_inner = TicToc()\n",
    "    t_inner.tic()\n",
    "\n",
    "    state = np.random.get_state()\n",
    "    np.random.seed(0)  # equivalent to MATLAB's rng('default') ?\n",
    "\n",
    "    # y_b = [row[i] for row in y]\n",
    "    y_b = y[:, i]\n",
    "\n",
    "    # Split data into train and test to verify accuracy after fitting the model\n",
    "    # Note: z is used here, not z_norm. If normalise and scale ahead, some test data might leak into\n",
    "    # the training process;\n",
    "    # x_train, x_test, y_train, y_test = train_test_split(z, y_b, \n",
    "    #                                                     test_size=0.25,\n",
    "    #                                                     random_state=42) # =0?\n",
    "    \n",
    "    # REQUIRE: Test case for validation the result --better!\n",
    "    skf = StratifiedKFold(n_splits = opts_csv_fold, shuffle = True, random_state = 0)\n",
    "    # skf = KFold(n_splits = opts_csv_fold, shuffle = True, random_state = 0)\n",
    "    \n",
    "    # Test k-fold cross validation\n",
    "    # data_splits = skf.split(z_norm, y_b)\n",
    "    # test_split = next(data_splits)\n",
    "    # print(\"Train indices: \", test_split[0][0:100])\n",
    "    # print(\"Test indices: \", test_split[1][0:100])\n",
    "\n",
    "    fit_matsvm(z, y_b, w[:, i], skf, kernel_fcn, np.nan)\n",
    "\n",
    "    best_svm, y_sub, p_sub, y_hat, p_hat, best_C, best_g = fit_matsvm(z, y_b, w[:, i], skf, kernel_fcn, np.nan)\n",
    "    aux = confusion_matrix(y_b, y_sub)\n",
    "    print(\"------------aux-----------\")\n",
    "    print(aux)\n",
    "    # 66    17   16   113\n",
    "\n",
    "    # np.prod(aux.shape) != 4 is False\n",
    "    cvcmat[i, :] = aux.flatten()\n",
    "    models_left = nalgos - (i + 1)\n",
    "    print(f\"    -> PYTHIA has trained a model for {algo_labels[i]}, there are {models_left} models left to train.\")\n",
    "\n",
    "    print(f\"      -> Elapsed time: {t_inner.tocvalue():.2f}s\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n",
      "[ 29.  72. 119.  20.  21.  14.  58.  78.  17.  16.]\n",
      "[0.67878788 0.89320388 0.67307692 0.76744186 0.78571429 0.76111111\n",
      " 0.98425197 0.8220339  0.74418605 0.70238095]\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "tn, fp, fn, tp = cvcmat[:, 0], cvcmat[:, 1], cvcmat[:, 2], cvcmat[:, 3]\n",
    "print(tn.dtype)  \n",
    "print(tn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "accuracy = (tp + tn) / ninst\n",
    "\n",
    "print(precision)\n",
    "# print(recall)\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: accuracy score = 0.8837209302325582\n",
      "1 fold: accuracy score = 0.9047619047619048\n",
      "2 fold: accuracy score = 0.8333333333333334\n",
      "3 fold: accuracy score = 0.8333333333333334\n",
      "4 fold: accuracy score = 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "y = pd.read_csv('./data/ybin.csv')\n",
    "y = y.values.tolist()\n",
    "\n",
    "z = pd.read_csv('./data/z.csv').values.tolist()\n",
    "z_norm = zscore(z, axis = 0, ddof = 1)\n",
    "\n",
    "for i in range(nalgos):\n",
    "    t_inner = TicToc()\n",
    "    t_inner.tic()\n",
    "\n",
    "    state = np.random.get_state()\n",
    "    np.random.seed(0)  # equivalent to MATLAB's rng('default') ?\n",
    "\n",
    "    # REQUIRE: Test case for validation the result\n",
    "    y_b = [row[i] for row in y]\n",
    "    # y_b = y[:, i]\n",
    "    skf = StratifiedKFold(n_splits = opts_csv_fold, shuffle = True, random_state = 0)\n",
    "    \n",
    "    kkv= dict()\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(np.zeros(len(y_b)), y_b)):\n",
    "        kkv[i] = [train_index.tolist(), test_index.tolist()]\n",
    "    \n",
    "    # start training using svm\n",
    "    svm_res = fit_libsvm(z_norm, y_b, kkv, kernel_fcn)\n",
    "\n",
    "    # visualise accuracy score\n",
    "for k, v in svm_res.items():\n",
    "    print(f'{k} fold: accuracy score = {v}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c8d4e9334dcc093c5455b6f55ea09f9028125097d0b5d072b5d027a0ea5ecbe2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.19 ('new_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
