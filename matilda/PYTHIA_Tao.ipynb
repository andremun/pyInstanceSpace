{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matilda'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatilda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlgorithmSummary\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatilda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moption\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PythiaOptions\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matilda'"
     ]
    }
   ],
   "source": [
    "# Import all the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "\n",
    "from numpy.typing import NDArray\n",
    "from pytictoc import TicToc\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from matilda.data.model import AlgorithmSummary\n",
    "from matilda.data.option import PythiaOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_Z = 'tests/pythia/test_pythia_input/z_M.csv'\n",
    "CSV_Y = 'tests/pythia/test_pythia_input/y.csv'\n",
    "CSV_YBIN = 'tests/pythia/test_pythia_input/ybin.csv'\n",
    "CSV_YBEST = 'tests/pythia/test_pythia_input/ybest.csv'\n",
    "CSV_ALGO = 'tests/pythia/test_pythia_input/algolabels.csv'\n",
    "\n",
    "pythia_opts = PythiaOptions(\n",
    "    cv_folds=5,\n",
    "    is_poly_krnl=False,\n",
    "    use_weights=False,\n",
    "    use_lib_svm=False,\n",
    ")\n",
    "\n",
    "z = pd.read_csv(CSV_Z, header=None, dtype=np.float64)\n",
    "y = np.loadtxt(CSV_Y, delimiter=',')\n",
    "y_bin = np.loadtxt(CSV_YBIN, delimiter=',', skiprows=1)\n",
    "y_best = np.loadtxt(CSV_YBEST, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pythia(\n",
    "        z: NDArray[np.double],\n",
    "        y: NDArray[np.double],\n",
    "        y_bin: NDArray[np.double],\n",
    "        y_best: NDArray[np.double],\n",
    "        algo_labels: list[str],\n",
    "        opts: PythiaOptions,\n",
    ") -> list[AlgorithmSummary]:\n",
    "\n",
    "    print(\" -> Initializing PYTHIA.\")\n",
    "\n",
    "    # Initialise data with its structure that can be used in Pythia.py\n",
    "    mu, sigma = np.mean(z, axis=0), np.std(z, ddof=1, axis=0)\n",
    "    z_norm = (z-mu)/sigma\n",
    "    ninst, nalgos = y_bin.shape\n",
    "    cp, svm = [None] * nalgos, [None] * nalgos\n",
    "    cvcmat = np.zeros((nalgos, 4))\n",
    "    y_sub, y_hat = np.zeros_like(y_bin, dtype=bool), np.zeros_like(y_bin, dtype=bool)\n",
    "    pr0_sub, pr0_hat = np.zeros_like(y_bin, dtype=bool), np.zeros_like(y_bin, dtype=float)\n",
    "    box_const, k_scale = np.zeros(nalgos), np.zero(nalgos)\n",
    "\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "    precalparams = (\n",
    "        hasattr(opts, 'params') and\n",
    "        isinstance(opts.params, (list, np.ndarray)) and\n",
    "        np.array(opts.params).shape == (nalgos, 2)\n",
    "    )\n",
    "    params = np.full((nalgos, 2), np.nan)\n",
    "\n",
    "    if opts.is_poly_krnl:\n",
    "        kernel_fcn = \"polynomial\"\n",
    "    else:\n",
    "        if ninst > 1000:\n",
    "            print(\"  -> For datasets larger than 1K Instances, PYTHIA works better with a Polynomial kernel.\")\n",
    "            print(\"  -> Consider changing the kernel if the results are unsatisfactory.\")\n",
    "            print(\"-------------------------------------------------------------------------\")\n",
    "        kernel_fcn = \"gaussian\"\n",
    "    print(\" => PYTHIA is using a \" + +\" kernel\")\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "    if opts.use_lib_svm:\n",
    "        print(\" -> Using LIBSVM's libraries\")\n",
    "\n",
    "        if precalparams:\n",
    "            print(\" -> Using pre-calculated hyper-parameters for the SVM.\")\n",
    "            params = opts.params\n",
    "        else:\n",
    "            print(\" -> Search on a latin hyper-cube design will be used for parameter hyper-tunning.\")\n",
    "    else:\n",
    "        print(\" -> Using MATLAB's SVM libraries.\")\n",
    "\n",
    "        if precalparams:\n",
    "            print(\" -> Using pre-calculated hyper-parameters for the SVM.\")\n",
    "            params = opts.params\n",
    "        else:\n",
    "            print(\" -> Bayesian Optimization will be used for parameter hyper-tunning.\")\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "        if opts.use_weights:\n",
    "            print(\" -> PYTHIA is using cost-sensitive classification.\")\n",
    "            w = np.abs(y - np.nanmean(y))\n",
    "            w [w == 0] = np.min(w [w != 0])\n",
    "            w [np.isnan(w)] = np.max(w [~np.isnan(w)])\n",
    "            w_aux = w\n",
    "        else:\n",
    "            print(\" -> PYTHIA is not using cost-sensitive classification.\")\n",
    "            w = np.ones((ninst, nalgos))\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "    print(\"  -> Using a \" + opts.cv_folds + \"-fold stratified cross-validation experiment to evaluate the SVMs.\")\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    print(\"  -> Training has started. PYTHIA may take a while to complete...\")\n",
    "\n",
    "    t = TicToc()\n",
    "    t.tic()\n",
    "\n",
    "    for i in range(nalgos):\n",
    "        t_inner = TicToc()\n",
    "        t_inner.tic()\n",
    "\n",
    "        np.random.seed(0)\n",
    "\n",
    "        y_b = y_bin[:, i]\n",
    "\n",
    "        cp[i] = StratifiedKFold(n_splits=opts.cv_folds, shuffle=True, random_state = 0)\n",
    "\n",
    "        if opts.use_lib_svm:\n",
    "            None# fit_libsvm(z_norm, y_b, cp[i], kernel_fcn, params[i])\n",
    "        else:\n",
    "            svm, y_sub, p_sub, y_hat, p_hat, c, g = fit_mat_svm(z_norm, y_b, w_aux[:, i], cp[i], kernel_fcn, params[i])\n",
    "\n",
    "\n",
    "        aux = confusion_matrix(y_b, y_sub[i])\n",
    "        if np.prod(aux.shape) != 4:\n",
    "            caux = aux\n",
    "            aux = np.zeros((2, 2))\n",
    "\n",
    "            if np.all(y_b == 0):\n",
    "                if np.all(y_sub[:, i] == 0):\n",
    "                    aux[0, 0] = caux\n",
    "                elif np.all(y_sub[:, i] == 1):\n",
    "                    aux[1, 0] = caux\n",
    "\n",
    "            elif np.all(y_b == 1):\n",
    "                if np.all(y_sub[:, i] == 0):\n",
    "                    aux[0, 1] = caux\n",
    "                elif np.all(y_sub[:, i] == 1):\n",
    "                    aux[1, 1] = caux\n",
    "\n",
    "        cvcmat[:, i] = aux.flatten()\n",
    "        models_left = nalgos - (i + 1)\n",
    "        print(\"    -> PYTHIA has trained a model for \" + algo_labels[i] + \", there are \" + models_left +\" models left to train.\")\n",
    "        print(\"    -> Elapsed time: \" + t_inner.tocvalue() +\"s\")\n",
    "\n",
    "    tn, fp, fn, tp = cvcmat[:, 0], cvcmat[:, 1], cvcmat[:, 2], cvcmat[:, 3]\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    accuracy = (tp + tn) / ninst\n",
    "\n",
    "    print(\"Total elapsed time: \" + t.tocvalue() + \"s\")\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    print(\" -> PYTHIA has completed training the models.\")\n",
    "    print(\" -> The average cross validated precision is: \" + np.round(100 * np.mean(precision), 1) + \"%\")\n",
    "    print(\" -> The average cross validated accuracy is: \" + np.round(100 * np.mean(accuracy), 1) + \"%\")\n",
    "    print(\"    -> Elapsed time: \" + t.tocvalue() + \"s\")\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    if nalgos > 1:\n",
    "        best, selection_0 = (\n",
    "            np.max(y_hat * precision.T, axis=1),\n",
    "            np.argmax(y_hat * precision.T, axis=1),\n",
    "        )\n",
    "    else:\n",
    "        best, selection_0 = y_hat, y_hat\n",
    "\n",
    "    default = np.argmax(np.mean(y_bin, axis=0))\n",
    "    selection_1 = selection_0.copy\n",
    "    selection_0[best <= 0] = 0\n",
    "    selection_1[best <= 0] = default\n",
    "\n",
    "    sel0 = selection_0[:, None] == np.arrange(1, nalgos + 1)\n",
    "    sel1 = selection_1[:, None] == np.arramge(1, nalgos + 1)\n",
    "    avgperf = np.nanmean(y)\n",
    "    stdperf = np.nanstd(y)\n",
    "    y_full = y.copy()\n",
    "    y_svms = y.copy()\n",
    "    y[~sel0] = np.NaN\n",
    "    y_full[~sel1] = np.NaN\n",
    "    y_svms[~y_hat] = np.NaN\n",
    "\n",
    "    pgood = np.mean(np.any(y_bin & sel1, axis=1))\n",
    "    fb = np.sum(np.any(y_bin & ~sel0, axis=1))\n",
    "    fg = np.sum(np.any(~y_bin & sel0, axis=1))\n",
    "    tg = np.sum(np.any(y_bin & sel0, axis=1))\n",
    "\n",
    "    precisionsel = tg / (tg + fg)\n",
    "    recallsel = tg / (tg + fb)\n",
    "\n",
    "    print(\"  -> PYTHIA is preparing the summary table.\")\n",
    "    summaries: list[AlgorithmSummary] = []\n",
    "\n",
    "    for i, label in enumerate(algo_labels + [\"Oracle\", \"Selector\"]):\n",
    "        summary = AlgorithmSummary(\n",
    "            label,\n",
    "            np.round(np.append(avgperf, [np.nanmean(y_best), np.nanmean(y_full)]), 3),\n",
    "            np.round(np.append(stdperf, [np.nanstd(y_best), np.nanstd(y_full)]), 3),\n",
    "            np.round(np.append(np.mean(y_bin, axis=0), [1, pgood]), 3),\n",
    "            np.round(np.append(np.nanmean(y_svms), [np.nan, np.nanmean(y)]), 3),\n",
    "            np.round(np.append(np.nanstd(y_svms), [np.nan, np.nanstd(y)]), 3),\n",
    "            np.round(np.append(100 * accuracy, [np.nan, np.nan]), 1),\n",
    "            np.round(np.append(100 * precision, [np.nan, precisionsel]), 1),\n",
    "            np.round(100 * np.append(recall, [np.nan, recallsel]), 1),\n",
    "            np.round(box_const, 3),\n",
    "            np.round(k_scale, 3),\n",
    "        )\n",
    "\n",
    "        summaries.append(summary)\n",
    "\n",
    "    print(\"  => PYTHIA has completed! Performance of the models:\")\n",
    "    print(\" \")\n",
    "\n",
    "    for result in summaries:\n",
    "        print(result)\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitmatsvm(\n",
    "        z_norm: NDArray[np.double],\n",
    "        y_bin: NDArray[np.double],\n",
    "        w_aux: NDArray[np.double],\n",
    "        cp: StratifiedKFold, #not sure\n",
    "        kernel_fcn: str,\n",
    "        params: NDArray[np.double],\n",
    "    ):\n",
    "\n",
    "    # Scikit-learn lib need to ensure data contiguity\n",
    "    z_norm = np.ascountiguousarray(z_norm)\n",
    "    y_bin = np.ascountiguousarray(y_bin)\n",
    "    w_aux = np.ascountiguousarray(w_aux)\n",
    "\n",
    "\n",
    "    # Check if parallel processing is available\n",
    "    n_workers = multiprocessing.cpu_count() if multiprocessing.cpu_count() > 1 else 1\n",
    "\n",
    "    if np.any(np.isnan(params)):\n",
    "        param_grid = {\n",
    "            \"C\": np.logspace(-10, 4, num=15, base=2),\n",
    "            \"gamma\": np.logspace(-10, 4, num=15, base=2),\n",
    "        }\n",
    "\n",
    "        svm_model = SVC(kernel=kernel_fcn, class_weight=None, random_state=0)\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=svm_model,\n",
    "            params_grid=param_grid,\n",
    "            cv = cp,\n",
    "            n_jobs=n_workers,\n",
    "            scoring=\"neg_log_loss\",\n",
    "            verbose=0,\n",
    "        )\n",
    "        grid_search.fit(z_norm, y_bin)\n",
    "\n",
    "        best_svm = grid_search.best_estimator_\n",
    "        c = grid_search.best_params_[\"C\"]\n",
    "        g = grid_search.best_estimator_[\"gamma\"]\n",
    "\n",
    "        y_sub = best_svm.predict(z_norm)\n",
    "        p_sub = best_svm.predict_proba(z_norm)[:, 1]\n",
    "\n",
    "        y_hat = y_sub\n",
    "        p_hat = p_sub\n",
    "\n",
    "    else:\n",
    "        c = params[0]\n",
    "        g = params[1]\n",
    "\n",
    "        svm_model = SVC(C=c, gamma=g, kernel=kernel_fcn, probability=True)\n",
    "        y_sub = np.zeros_like(y_bin)\n",
    "        p_sub = np.zeros_like(y_bin, dtype=float)\n",
    "\n",
    "        for train_index, test_index in cp.split(z_norm, y_bin):\n",
    "            svm_model.fit(z_norm[train_index], y_bin[train_index], sample_weight = w_aux[train_index])\n",
    "            y_sub[test_index] = svm_model.predict(z_norm[test_index])\n",
    "            p_sub[test_index] = svm_model.predict(z_norm[test_index])[:,1]\n",
    "\n",
    "        svm_model.fit(z_norm, y_bin, sample_weight=w_aux)\n",
    "        y_hat = svm_model.predict(z_norm)\n",
    "        p_hat = svm_model.predict(z_norm)[:, 1]\n",
    "\n",
    "    return best_svm, y_sub, p_sub, y_hat, p_hat, C, g"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
